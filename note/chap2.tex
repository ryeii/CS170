\section{Divide-and-conquer Algorithms}

\subsection{Multiplication}

\begin{definition}[Integer Multiplication]
A divide-and-conquer algorithm for integer multiplication is defined as follows:
\begin{lstlisting}[mathescape=true, language=Octave]
function mul(x(0b[1...k]), y(0b[1...h]))
	%Input: Positive integers x, y in binary
	%Output: x times y

	n = $\max$(size of x, size of y)
	if n == 1: return $x\times y$

	$x_L$, $x_R$ = x(0b[1...$\lceil n/2\rceil$]), x(0b[$\lfloor n/2\rfloor$...n])
	$y_L$, $y_R$ = y(0b[1...$\lceil n/2\rceil$]), y(0b[$\lfloor n/2\rfloor$...n])

	$P_1 = mul(x_L, y_L)$
	$P_2 = mul(x_R, y_R)$
	$P_3 = mul(x_L+x_R, y_L+y_R)$
	return $P_1\times 2^n + (P_3-P_1-P_2)\times 2^{n/2}+P_2$
\end{lstlisting}
Where $0b[1...k]$ denotes the binary string representing a number.
\end{definition}
Each call of \textit{mul} has three recursive calls, inputs of which are half the size of the original inputs, and the base cases (x times y) take constant time. Therefore we conclude that the time taken by this algorithm is
\[
T(n) = 3T(n/2)+O(n)
\]
Apply the Master Algorithm in Chap 2.b, we conclude that the time complexity of this algorithm is
\[
T(n) \in \Theta(n^{\log_23}) \approx \Theta(n^{1.585})
\]

\subsection{Recurrence Relations}

\begin{theorem}[Master Algorithm]
If $T(n) = aT(n/b) + cn^k$ and $T(1) = c$ for some constants $a$, $b$, $c$ and $k$, then
\begin{equation*}
T(n) \in \begin{cases}
\Theta(n^k) &if \; a<b^k \\
\Theta(n^k\log n) &if \; a=b^k \\
\Theta(n^{\log_ba}) &if \; a>b^k
\end{cases}
\end{equation*}
\end{theorem}

\subsection{Mergesort}

\begin{definition}[Mergesort]
The Mergesort algorithm is defined as follows:
\begin{lstlisting}[mathescape=true, language=Octave]
function mergesort(a[1...n])
	%Input: An array of numbers a[1...n]
	%Output: Sorted array a

	if n>1:
		return merge(mergesort(a[1...$\lfloor n/2 \rfloor$]), mergesort(a[$\lfloor n/2 \rfloor$+1...n]))
	else:
		return a

function merge(x[1...k], y[1...h])
	%Input: Two arrays of numbers (x[1...k], y[1...h])
	%Output: An array of numbers in x and y in ascending order

	if k=0: return y
	if l=0: return x
	if x[1] <= y[1]:
		return x[1] $\circ$ merge(x[2...k], y[1...h])
	else:
		return y[1] $\circ$ merge(x[1...k], y[2...h])
\end{lstlisting}
Where $\circ$ denotes concatenation.
\end{definition}

The \textit{merge} function above does a constant amount of work (concatenating two arrays) per recursive call, for a total running time of $O(k + h)$. Thus the calls to \textit{merge} in \textit{mergesort} are linear, we conclude that the overall time taken by \textit{mergesort} is
\[
T(n) = 2T(n/2) + O(n)
\]
Recall the Master Algorithm in Chap 2.b, we conclude that the time complexity of this algorithm is
\[
T(n) \in \Theta(n\log n)
\]

\begin{remark}
$n\log n$ is the lower bound for sorting, and therefore \textit{mergesort} is optimal.

\begin{proof}
Sorting algorithms can be depicted as trees that each non-leaf node represents a comparison between two elements, and each leaf denotes a permutation of the input array (and thus a binary search tree since each non-leaf nodes have two children). Consider such tree that sorts an array $a[1...n]$. The total number of the leaves is $n!$. A binary tree of depth $d$ has at most $2^d$ leaves. Therefore, the depth of the tree and the complexity of this algorithm should be at least $\log(n!)$, which is the worst case of this algorithm. Since $\log(n!)\le cn\log(n)$, we conclude that $n\log(n)$ is optimal for sorting algorithms.
\end{proof}
\end{remark}

\subsection{Medians}

\begin{definition}[selection]
A randomized divide-and-conquer algorithm for selection is defined as follows

\end{definition}

\subsection{Matrix Multiplication}

\begin{definition}

\end{definition}

\subsection{Fast Fourier Transform}